{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers\n",
    "\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"stanfordnlp/sst2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78703e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example):\n",
    "    return tokenizer(\n",
    "        example[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(tokenize_fn, batched=True)\n",
    "tokenized.set_format(type=\"tensorflow\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79dcd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tf(ds, batch_size=32, shuffle=True):\n",
    "    features = {\n",
    "        \"input_ids\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "        \"attention_mask\": tf.TensorSpec(shape=(128,), dtype=tf.int32),\n",
    "    }\n",
    "    generator = lambda: (\n",
    "        ({\"input_ids\": x[\"input_ids\"], \"attention_mask\": x[\"attention_mask\"]}, x[\"label\"])\n",
    "        for x in ds\n",
    "    )\n",
    "    tf_dataset = tf.data.Dataset.from_generator(generator, output_signature=(features, tf.TensorSpec(shape=(), dtype=tf.int64)))\n",
    "    if shuffle:\n",
    "        tf_dataset = tf_dataset.shuffle(1000)\n",
    "    return tf_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = to_tf(tokenized[\"train\"])\n",
    "val_ds = to_tf(tokenized[\"validation\"], shuffle=False)\n",
    "test_ds = to_tf(tokenized[\"test\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a8515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embed_dim, max_length):\n",
    "        super().__init__()\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=max_length, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        positions = tf.range(start=0, limit=tf.shape(x)[-1], delta=1)\n",
    "        return self.token_emb(x) + self.pos_emb(positions)\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ff_dim, activation='relu'),\n",
    "            tf.keras.layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        out1 = self.layernorm1(inputs + self.dropout1(attn_output, training=training))\n",
    "        ffn_output = self.ffn(out1)\n",
    "        return self.layernorm2(out1 + self.dropout2(ffn_output, training=training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500dc6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer_model(vocab_size, max_len=128, embed_dim=512, num_heads=8, ff_dim=2048, num_layers=8):\n",
    "    input_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "    x = PositionalEmbedding(vocab_size, embed_dim, max_len)(input_ids)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    outputs = tf.keras.layers.Dense(2, dtype='float32')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs={\"input_ids\": input_ids, \"attention_mask\": attention_mask}, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bf844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "model = build_transformer_model(vocab_size=vocab_size)\n",
    "\n",
    "class WarmUpLinearDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, base_lr, warmup_steps, total_steps):\n",
    "        super().__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup = self.base_lr * (step / self.warmup_steps)\n",
    "        decay = self.base_lr * (1.0 - (step - self.warmup_steps) / (self.total_steps - self.warmup_steps))\n",
    "        return tf.cond(step < self.warmup_steps, lambda: warmup, lambda: decay)\n",
    "\n",
    "\n",
    "steps_per_epoch = 2000\n",
    "epochs = 10\n",
    "total_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "\n",
    "lr_schedule = WarmUpLinearDecay(base_lr=3e-5, warmup_steps=warmup_steps, total_steps=total_steps)\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=1e-4)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"best_model.keras\", save_best_only=True, monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=10, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a91031",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"saved_transformer_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0aff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"saved_transformer_model\", custom_objects={\n",
    "    \"PositionalEmbedding\": PositionalEmbedding,\n",
    "    \"TransformerBlock\": TransformerBlock\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee37c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = loaded_model.evaluate(test_ds)\n",
    "print(f\"Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed81f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    logits = loaded_model({\"input_ids\": tokens[\"input_ids\"], \"attention_mask\": tokens[\"attention_mask\"]})\n",
    "    pred = tf.argmax(logits, axis=1).numpy()[0]\n",
    "    label = \"positive\" if pred == 1 else \"negative\"\n",
    "    print(f\"Prediction: {label} (class {pred})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38140bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(\"This movie was surprisingly great!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
